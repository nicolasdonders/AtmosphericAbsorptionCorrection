{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d301cadd-4b59-4658-a57b-60a204ba2bc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __Model for Ultraviolet Atmospheric Absorption (Version 9.1)__\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff8e50-7e1e-404d-9eca-3b768d533599",
   "metadata": {},
   "source": [
    "* Author: Nicolas Donders\n",
    "* Revision History:\n",
    "    * V1 (Oct 2021): Set up simple model and results\n",
    "    * V2 (Nov 2021): Importing UVVIS data from MPI Mainz\n",
    "    * V3 (Feb 2022): Utilizing a spherical shells atmospheric model to modulate optical depth calculation. Importing HRTS spectra and folding this into FURST instrument and CCD camera.\n",
    "    * V4 (Jun 2022): Fixed missing spectral plate-scale factor. Cleaned up and fixed/added more comments.\n",
    "    * V5 (Jul 2022): Focus inversion on absorption spectrum rather than density profile.\n",
    "    * V6 (Aug 2022): Impliment raw SR data by simplifying code.\n",
    "    * V6.1 (Aug 2022): Rewriting for different spectral range and altitudes.\n",
    "    * V7.1 (Nov 2022): Applying to MaGIXS spectral range. \n",
    "    * V7.2 (Jan 2023): Trimmed for Easier use, looked at many launch times to predict effect on MaGIXS missions at various months of the year.\n",
    "    * V8 (Apr 2023): Adjusted for 19.3 nm (Hi-C 1) and 17.2 nm (Hi-C 2.1). Produced 2D pictures saved in FITS files. Also played with generating predictions for Hi-C flare (which launched Spring 2024).\n",
    "    * V9 (Feb 2025): Modifications for generating publication plots. Moved molar mass data outside of generating functions, part of input now.\n",
    "    * V9.1 (Mar 2025): Simplifying code for export to P.S. Athray for use with future mission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cff736-89ae-4c04-8ba5-f9b18f0962ca",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 1: Initialize Code with imports and basic user-defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f554c9-370c-4000-bc0c-614378500eb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df945874-901b-45c1-9fba-a3581deb0739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def install_package(package_name):\n",
    "    try:\n",
    "        # Attempt to import the package\n",
    "        __import__(package_name)\n",
    "    except ImportError:\n",
    "        # If the package is not installed, install it using pip\n",
    "        try:\n",
    "            subprocess.check_call(['python', '-m', 'pip', 'install', package_name])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error installing {package_name}: {e}\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "packages = ['requests','bs4','datetime','matplotlib','nrlmsise00[dataset]','numpy','os','pandas','pysolar','pytz','scipy']\n",
    "for pack in packages:\n",
    "    if install_package(pack):\n",
    "        print('package {} installed successfully'.format(pack))\n",
    "    else:\n",
    "        print('Failed to install {} package'.format(pack))\n",
    "\n",
    "import pytz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests as pull\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from scipy.interpolate import interp1d\n",
    "from nrlmsise00.dataset import msise_4d\n",
    "from pysolar.solar import get_altitude as zen_func\n",
    "from scipy.interpolate import pchip_interpolate as spline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d6b1b-48ef-4071-8acc-f50519bc84d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.2: User Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0cbc7-569f-436b-a09e-9a3a8728d434",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rolling_avg(array, window_size):\n",
    "    return np.convolve(array, np.ones(window_size)/window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a81906-7e86-48e2-9972-8ffd33cda991",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_continuum(array, window_size):\n",
    "    # Create an array of minimum values in each window\n",
    "    continuum = np.array([np.min(array[max(i - window_size // 2, 0):min(i + window_size // 2 + 1, len(array))])\n",
    "                         for i in range(len(array))])\n",
    "    # Apply rolling average to the minimum value array\n",
    "    continuum = np.convolve(continuum, np.ones(window_size) / window_size, mode='valid')\n",
    "    return continuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e2f13-465f-4edb-a9ae-ddedc1ab962e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def elevation_function(lat, lon):\n",
    "    \n",
    "    # Query USGS Elevation Point Service to get elevation in kilometers    \n",
    "    url = 'https://epqs.nationalmap.gov/v1/json?x={}&y={}&units=Meters&wkid=4326&includeDate=False'\\\n",
    "          .format(lon,lat)\n",
    "    elevation = pull.get(url).text.split('\"')[17]\n",
    "    \n",
    "    return float(elevation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31ea94-b05d-4c7a-af98-31ddb2387184",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_earthradius(lat):\n",
    "    # Earth radius values at Equator and Poles\n",
    "    r1 = 6378.1370\n",
    "    r2 = 6356.7523\n",
    "    \n",
    "    # Calculation of Earth radius at given latitude\n",
    "    a = (r1**2 * np.cos(lat))**2 + (r2**2 * np.sin(lat))**2\n",
    "    b = (r1 * np.cos(lat))**2 + (r2 * np.sin(lat))**2\n",
    "    \n",
    "    # Returns radius of Sea Level at given Latitude.\n",
    "    return np.sqrt(a / b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa649bb4-4558-4928-8a20-d2c14e669f3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_absorp_spline(absorp_data_all,waves,map_temp, n_mols, n_waves, n_height):\n",
    "    \n",
    "    header = ['Molecule', 'Wavelength (nm)', 'Temperature (K)', 'Absorption Cross Section (cm^2 molec^-1)']\n",
    "    mols_data = absorp_data_all[header[0]] # Molecules\n",
    "    absorp_spline = np.zeros((n_mols,n_waves,n_height))\n",
    "    \n",
    "    for m in range(n_mols):\n",
    "\n",
    "        molindex    = np.where(mols_data==mols[m])\n",
    "        waves_data  = absorp_data_all[header[1]].to_numpy()[molindex]\n",
    "        temp_data   = absorp_data_all[header[2]].to_numpy()[molindex]\n",
    "        absorp_data = absorp_data_all[header[3]].to_numpy()[molindex]\n",
    "\n",
    "        for i in range(len(waves_data)):\n",
    "            wi = (abs(waves_data[i]-waves)).argmin()\n",
    "            ti = (abs(temp_data[i]-map_temp)).argmin()\n",
    "            checkti = np.where(map_temp==map_temp[ti])[0]\n",
    "            if len(checkti)>1: ti=checkti\n",
    "            absorp_spline[m,wi,ti] = absorp_data[i]\n",
    "\n",
    "        for i in range(len(waves)):\n",
    "            Nnonzero = np.count_nonzero(absorp_spline[m,i,:])\n",
    "            if Nnonzero > 0:\n",
    "                index_nonzero = np.array(absorp_spline[m,i,:].nonzero(), dtype=int)[0] \n",
    "                if len(index_nonzero)==1:\n",
    "                    absorp_spline[m,i,:] = absorp_spline[m,i,index_nonzero]\n",
    "                if len(index_nonzero)>1:\n",
    "                    ti,tf = index_nonzero[0], index_nonzero[-1]+1\n",
    "                    f = interp1d(map_temp[index_nonzero], absorp_spline[m,i,index_nonzero])\n",
    "                    absorp_spline[m,i,ti:tf] = f(map_temp[ti:tf])\n",
    "                    \n",
    "        for j in range(len(map_temp)):\n",
    "            Nnonzero = np.count_nonzero(absorp_spline[m,:,j])\n",
    "            if Nnonzero > 0:\n",
    "                index_nonzero = np.array(absorp_spline[m,:,j].nonzero(), dtype=int)[0] \n",
    "                if len(index_nonzero)==1:\n",
    "                    absorp_spline[m,:,j] = absorp_spline[m,index_nonzero,j]\n",
    "                if len(index_nonzero)>1:\n",
    "                    wi,wf = index_nonzero[0], index_nonzero[-1]+1\n",
    "                    f = interp1d(waves[index_nonzero], absorp_spline[m,index_nonzero,j])\n",
    "                    absorp_spline[m,wi:wf,j] = f(waves[wi:wf])\n",
    "    return absorp_spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3682c9b-0855-41ba-ad21-f5afd0f2ffe2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_DiffPathLength(ri_0, r_0, d_height, cosmu, N):\n",
    "    DiffPathLength = np.zeros(N)\n",
    "    for z in range(N):\n",
    "        a = r_0 + d_height*float(z+1)\n",
    "        b = r_0 + d_height*float(z)\n",
    "        c = 1.0 # density_all_ratio[m,zz+z]\n",
    "        d = ri_0[m] / ( c*(ri_0[m]-1) + 1 )\n",
    "        mu = np.sqrt( 1 - ( (r_0+d_height)**2 / a**2 ) * d**2 * (1-cosmu**2) )\n",
    "        DiffPathLength[z] = (np.sqrt( a**2 - b**2 * (1-mu**2) ) - b*mu)*1e5\n",
    "    return DiffPathLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9272b8-c167-455c-95ae-410e94819ed1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_OptDepth(absorp, ri_0, density_all_ratio, d_height, cosmu):\n",
    "    n_mols = absorp.shape[0]\n",
    "    n_waves = absorp.shape[1]\n",
    "    n_height = absorp.shape[2]\n",
    "    OptDepth = np.zeros((n_waves, n_height))\n",
    "    for m in range(n_mols):\n",
    "        for z in range(n_height):\n",
    "            OptDepth[:,z] += np.sum( absorp[m,:,z:]*DiffPathLength[z:], axis=1)\n",
    "    return OptDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5adb180-df2c-4d6f-892e-6ef4a0a9c047",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getdata_xray(label,minmax,molmass):\n",
    "\n",
    "    # Convert from nm to keV\n",
    "    minkev = 1.24 / minmax[1] # 1.24 keV*nm\n",
    "    maxkev = 1.24 / minmax[0]\n",
    "    minkev = np.floor(minkev*10)/10\n",
    "    maxkev = np.ceil(maxkev*10)/10\n",
    "\n",
    "    # Set up Pandas DataFrame for output\n",
    "    headers = ['Molecule', 'Wavelength (nm)', 'Temperature (K)', 'Absorption Cross Section (cm^2 molec^-1)']\n",
    "    Data_out = pd.DataFrame(columns=headers)\n",
    "    \n",
    "    # Loop over each molecule in the label list\n",
    "    for m in range(len(label)):\n",
    "        \n",
    "        # Query NIST X-ray database and pull data\n",
    "        url = f'https://physics.nist.gov/cgi-bin/ffast/ffast.pl?Formula={label[m]}&gtype=2&range=S&lower={minkev}&upper={maxkev}&density=&frames=no&htmltable=1'\n",
    "        grab = pull.get(url)\n",
    "        soup0 = BS(grab.text, 'html.parser').get_text()\n",
    "        soup = soup0.split('cm2g-1')[1].split('\\n')\n",
    "        \n",
    "        # Create array to store data\n",
    "        Data = np.zeros((2, len(soup)-1), dtype=float)\n",
    "        for i in range(len(soup)-1):\n",
    "            Data[0, i] = soup[i].split('\\xa0\\xa0')[0] # keV\n",
    "            Data[1, i] = soup[i].split('\\xa0\\xa0')[1] # cm^2 * g^-1\n",
    "        Data[0, :] = 1.24 / Data[0, :]\n",
    "        Data[1, :] = Data[1, :] * molmass[m] # cm^2 * molecule^-1\n",
    "        Data = Data[:, ::-1]\n",
    "\n",
    "        # Get range of data to keep based on minmax range\n",
    "        if minmax != None:\n",
    "            ji = (abs(minmax[0] - Data[0, :])).argmin()\n",
    "            jf = (abs(minmax[1] - Data[0, :])).argmin()\n",
    "        else:\n",
    "            ji = 0\n",
    "            jf = n_data - 1\n",
    "\n",
    "        # Add data to output DataFrame\n",
    "        for j in range(ji, jf):\n",
    "            Data_out = Data_out.append({headers[0]: label[m], headers[1]: Data[0, j], headers[2]: 273, headers[3]: Data[1,j] }, ignore_index=True)\n",
    "        \n",
    "    return Data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0a3c7-de76-4a03-82ba-5d2e43e2d289",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getdata_uvvis(label,**kwargs):\n",
    "    \n",
    "    # Check to see if text string can be a float \n",
    "    def isfloat(string):\n",
    "        try:\n",
    "            float(string)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    # Parse through HTML in the URL to pull out hyperlinks\n",
    "    def htmlparser(urls,subtext):\n",
    "        # MPI Mainz UVVIS website\n",
    "        url = 'https://uv-vis-spectral-atlas-mainz.org/uvvis/'\\\n",
    "            + subtext\n",
    "        \n",
    "        # Grab all hyperlinks on this website\n",
    "        grab = pull.get(url)\n",
    "        soup = BS(grab.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            urls.append(link.get('href'))\n",
    "            \n",
    "        # Output the resulting list\n",
    "        return urls\n",
    "    \n",
    "    # Pull together all databases\n",
    "    def geturl_mols(mols):\n",
    "    \n",
    "        # Find all cross-section URLs\n",
    "        find = 'cross_sections/'\n",
    "        urls = htmlparser([],find)\n",
    "        urls = [match for match in urls if find in match][1:]\n",
    "\n",
    "        # Sort through each cross-section URL and list all molecule filenames\n",
    "        urls_sub = []\n",
    "        for u in range(len(urls)):\n",
    "            urls_sub = htmlparser(urls_sub,urls[u])\n",
    "\n",
    "        # Sort through list of filenames and grab the ones we care about\n",
    "        data_urls = [None]*len(mols)\n",
    "        for m in range(len(mols)):\n",
    "            data_urls[m] = [match for match in urls_sub if '/'+mols[m]+'.spc' in match][0]\n",
    "\n",
    "        return data_urls\n",
    "    \n",
    "    minmax = kwargs.get('minmax')\n",
    "    label = np.sort(label)\n",
    "    # Database URL prefix\n",
    "    url = 'https://uv-vis-spectral-atlas-mainz.org/uvvis_data/'\n",
    "    \n",
    "    # Set up Pandas Dataframe for output\n",
    "    headers = ['Molecule', \\\n",
    "               'Wavelength (nm)', \\\n",
    "               'Temperature (K)', \\\n",
    "               'Absorption Cross Section (cm^2 molec^-1)']\n",
    "    Data_out = pd.DataFrame(columns=headers)\n",
    "    molmass_out = [] #np.zeros(len(mols))\n",
    "    \n",
    "    # Finding all URLs associated with desired molecules\n",
    "    print('Gather Databases for: {}'.format(', '.join(label)))\n",
    "    data_urls = geturl_mols(label)\n",
    "    \n",
    "    for m in range(len(label)):\n",
    "\n",
    "        # Skipping certain files that aren't formatting yet\n",
    "        if label[m] == 'O2':\n",
    "            # skips = [11,23,24,25,26,38,42,48,63,81,87,88,89,119,125,126,141,143,208]\n",
    "            skips = [12,24,87,126,132,133]\n",
    "        else:\n",
    "            if label[m] == 'N2':\n",
    "                # skips = [4,6]\n",
    "                skips = [6]\n",
    "            else:\n",
    "                skips = []\n",
    "        \n",
    "        # Find all text-file URLs under the molecules' specific database\n",
    "        urls = htmlparser([],data_urls[m])\n",
    "        urls = [match for match in urls if \".txt\" in match]\n",
    "\n",
    "        # Go through each file with parallel optimization\n",
    "        Nfiles = len(urls)\n",
    "        for i in range(Nfiles):\n",
    "            \n",
    "            # Figure out data structure from filenames\n",
    "            \n",
    "            filename = url+urls[i]\n",
    "            temp_str = filename.split('K_')[0].split(')_')[1].split('-')\n",
    "            wave_str = filename.split('K_')[1].split('nm')[0].split('-')\n",
    "                        \n",
    "            # If range is specified, find min and max waves \n",
    "            # and skip files outside desired range\n",
    "            if minmax != None:\n",
    "                if isfloat(wave_str[0]):\n",
    "                    waves_min = float(wave_str[0])\n",
    "                    waves_max = float(wave_str[len(wave_str)-1])\n",
    "                else:\n",
    "                    waves_min = float(wave_str[0].split(',')[0])\n",
    "                    waves_max = float(wave_str[0].split(',')[1])\n",
    "                \n",
    "                if waves_max<minmax[0] \\\n",
    "                or waves_min>minmax[1]:\n",
    "                    skips.append(i)\n",
    "            \n",
    "            # Skip files as requested\n",
    "            # Will change to be either automatically detected or not needed            \n",
    "            if i not in skips:\n",
    "\n",
    "                # Import data, strip whitespace, and remove headers\n",
    "                Data_str = pd.read_table(filename,\\\n",
    "                                         header='infer',\\\n",
    "                                         encoding='ISO-8859-1', \\\n",
    "                                         index_col=False,\\\n",
    "                                         keep_default_na=False,\\\n",
    "                                         skip_blank_lines=True)\n",
    "                if isfloat(list(Data_str.head())[0].split()[0]):\n",
    "                    Data_str = pd.read_table(filename,\\\n",
    "                                             header=None,\\\n",
    "                                             encoding='ISO-8859-1', \\\n",
    "                                             index_col=False,\\\n",
    "                                             keep_default_na=False,\\\n",
    "                                             skip_blank_lines=True)\n",
    "                Data_str.dropna(inplace=True)\n",
    "                n_data = Data_str.shape[0]\n",
    "\n",
    "                # If there are two rows of headers, drop the leftover row\n",
    "                if isfloat(list(Data_str.iloc[0].dropna())[0]) == False:\n",
    "                    Data_str = Data_str.drop(0,axis=0)\n",
    "                if isfloat(list(Data_str.iloc[0].dropna())[0]) == False:\n",
    "                    Data_str = Data_str.iloc[1:,:]\n",
    "\n",
    "                # Split Data to two columns if it isn't already\n",
    "                n_data = Data_str.shape[0]\n",
    "                if Data_str.shape[1] == 1:\n",
    "                    Data_str.insert(1,1,[None]*n_data)\n",
    "                if isfloat(Data_str.iloc[0,0]) == False:\n",
    "                    for r in range(n_data):\n",
    "                        Data_str.iloc[r,1] = Data_str.iloc[r,0].split()[1]\n",
    "                        Data_str.iloc[r,0] = Data_str.iloc[r,0].split()[0]  \n",
    "                # print(Data_str)\n",
    "\n",
    "                n_data = len(Data_str.iloc[:,0])\n",
    "\n",
    "                # Splitting apart plus/minus symbols\n",
    "                for r in range(n_data):\n",
    "                    if isfloat(Data_str.iloc[r,1]) == False:\n",
    "                        if Data_str.iloc[r,1][0] == '<':\n",
    "                            # print('Fixing ',Data_str.iloc[r,1])\n",
    "                            Data_str.iloc[r,1] \\\n",
    "                                = Data_str.iloc[r,1]\\\n",
    "                                  .replace('<','')\n",
    "                            # print('Fixed ',Data_str.iloc[r,1])\n",
    "                        else:\n",
    "                            # print('Fixing ',Data_str.iloc[r,1])\n",
    "                            Data_str.iloc[r,1] \\\n",
    "                                = Data_str.iloc[r,1]\\\n",
    "                                  .replace(' ','')\n",
    "                            strfix0 \\\n",
    "                                = Data_str.iloc[r,1]\\\n",
    "                                  .split('(')[1]\\\n",
    "                                  .split(u'\\u00B1')[0]\n",
    "                            strfix1 \\\n",
    "                                = Data_str.iloc[r,1]\\\n",
    "                                  .split(')')[1]           \n",
    "                            Data_str.iloc[r,1] \\\n",
    "                                = strfix0+strfix1\n",
    "                            # print('Fixed',Data_str.iloc[r,1])\n",
    "\n",
    "                # Removing Error Limits / Extra columns\n",
    "                Data_str = Data_str.iloc[:,0:2]\n",
    "                \n",
    "                # Converting to floats\n",
    "                Data = np.asarray(Data_str,dtype=float)\n",
    "                \n",
    "                # Saving Cross-sections\n",
    "                cross = Data[:,1]\n",
    "                \n",
    "                #Sorting out data into dataframe output\n",
    "                n_data = len(Data)\n",
    "                if len(temp_str)==1:\n",
    "                    waves  = Data[:,0]\n",
    "                    temp   = np.zeros((n_data))+float(temp_str[0])\n",
    "                else:\n",
    "                    if len(wave_str)==1:\n",
    "                        temp  = Data[:,0]\n",
    "                        waves = np.zeros((n_data))+float(wave_str[0])\n",
    "                    else:\n",
    "                        waves  = Data[:,0]\n",
    "                        temp   = np.zeros((n_data)) \\\n",
    "                               + (float(temp_str[0])+float(temp_str[1]))/2.\n",
    "                \n",
    "                # Only append data within desired range\n",
    "                if minmax != None:\n",
    "                    ji = (abs(minmax[0]-waves)).argmin()\n",
    "                    jf = (abs(minmax[1]-waves)).argmin()\n",
    "                else:\n",
    "                    ji = 0\n",
    "                    jf = n_data-1\n",
    "                    \n",
    "                for j in range(ji,jf):\n",
    "                    Data_out = Data_out.append({headers[0]: label[m],      \\\n",
    "                                                headers[1]: waves[j],   \\\n",
    "                                                headers[2]: temp[j],    \\\n",
    "                                                headers[3]: cross[j] }, \\\n",
    "                                                ignore_index=True       )\n",
    "                \n",
    "            # Printing run progress\n",
    "            print(\"File {:.0f} of {:.0f} for \\t {}: \\t {:.1f}%\" \\\n",
    "                  .format(i+1, Nfiles, label[m], (i+1)/Nfiles*100), end='\\r')\n",
    "            \n",
    "        # Printing final progress\n",
    "        # print('\\t \\t \\t \\t', end='\\r')\n",
    "        print('\\nData pts added for \\t {}: \\t {:.0f}'\\\n",
    "              .format(label[m],Data_out[Data_out[Data_out.columns[0]]==label[m]].shape[0]))\n",
    "    \n",
    "    # Sort output by molecule, then by wavelength, then by temperature, then by cross-section\n",
    "    Data_out = Data_out.sort_values([Data_out.columns[0],Data_out.columns[1],Data_out.columns[2],Data_out.columns[3]])\n",
    "    \n",
    "    # Plot output, if desired\n",
    "    if kwargs.get('plot') is True:\n",
    "        groups = Data_out.groupby(Data_out.columns[0])\n",
    "        aspect,pt = (8,4),12\n",
    "        plt.style.use('default')\n",
    "        plt.figure(figsize=aspect)\n",
    "        plt.rcParams.update({'font.size': pt})\n",
    "        for mol,group in groups:\n",
    "            plt.scatter(group[Data_out.columns[1]],group[Data_out.columns[3]],\\\n",
    "                        # c=group[Data_out.columns[2]],cmap='plasma',\\\n",
    "                        lw=1,label=mol)\n",
    "        plt.yscale('log')\n",
    "        if minmax != None: plt.xlim(minmax[0],minmax[1])\n",
    "        else: plt.xlim(Data_out[Data_out.columns[1]].min(),Data_out[Data_out.columns[1]].max())\n",
    "        plt.xlabel(Data_out.columns[1])\n",
    "        plt.ylabel(Data_out.columns[3])\n",
    "        # plt.colorbar(label=Data_out.columns[2])\n",
    "        plt.legend()\n",
    "    \n",
    "    if kwargs.get('save') is True:\n",
    "        if kwargs.get('minmax') != None:\n",
    "            savefilename = 'Donders_mols_{}_range_{}_{}nm.csv'\\\n",
    "                            .format('_'.join(label),minmax[0],minmax[1])\n",
    "        else:\n",
    "            savefilename = 'Donders_mols_{}_range_{}_{}nm.csv'\\\n",
    "                            .format('_'.join(label),int(Data_out[Data_out.columns[1]].min()),int(Data_out[Data_out.columns[1]].max()))\n",
    "        Data_out.to_csv(savefilename, index=False)\n",
    "        print('Saved file as {}'.format(savefilename))\n",
    "    \n",
    "    # Return output data\n",
    "    return Data_out, molmass_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded7690-5a45-4a23-a2fd-e74922072fad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.3 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea766d5-6317-4f78-9f8e-e093ac4155b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NA = 6.022E23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300e6e1-2c29-439b-a332-247d5d5d2330",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2: Initial Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756315a2-4d0d-45ca-b9e7-45411f8fc11c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Flight Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382e96a-332e-49b6-809d-f9681dd5f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timezone = pytz.timezone(\"UTC\")   # Set timezone to UTC\n",
    "launch_date = [2012, 7, 11]       # Date of launch [year, month, day]\n",
    "launch_time = [18,0,0]            # time of start [hours,minutes,seconds]\n",
    "lat, lon = 32.417776, -106.321547 # Define latitude and longitude (WSMR = 32.4 N, -106.3 W)\n",
    "\n",
    "# File with flight profile, can be csv or txt, but should be \n",
    "# first column as time after launch (in seconds) and \n",
    "# second column as altitude (in km).\n",
    "# flight_profile_file = 'test.csv'\n",
    "flight_profile_file = 'test_profile.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a30dc7-7df7-4f70-8626-5322487f9f4d",
   "metadata": {},
   "source": [
    "## 2.2 Science Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0432ac3-f965-4dbe-b149-ad2405e89730",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set minimum and maximum wavelength range and step size\n",
    "min_wave, max_wave = 19.3-5,19.3+5 # nm\n",
    "d_wave = 0.01 # nm, could be based on camera pixels if applicable\n",
    "\n",
    "# List of molecules to consider in atmospheric model\n",
    "mols = ['O', 'O2', 'N', 'N2', 'Ar', 'He', 'H']\n",
    "molmass_g = [15.999, 31.999, 14.0067, 28.0134, 39.948, 4.002602, 1.00784] # in g/mol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9e2db-271b-4e0c-a6c4-567a6fa48af6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3: Calculated Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a56a2-bd4e-412f-9792-85a11a853f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1: Flight Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0248bb-0a0e-4d91-9029-5a1a456eed62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Flight Profile from directory\n",
    "fp_tall = np.loadtxt(flight_profile_file, skiprows=1, usecols=[0], delimiter='\\t')\n",
    "fp_zall = np.loadtxt(flight_profile_file, skiprows=1, usecols=[1], delimiter='\\t')\n",
    "\n",
    "#Define flight date and time with UTC timezone\n",
    "start_time = launch_time[0] + launch_time[1]/60 + launch_time[2]/60/60\n",
    "flight_datetime = datetime(launch_date[0], launch_date[1], launch_date[2],\n",
    "                           launch_time[0], launch_time[1], launch_time[2],\n",
    "                           tzinfo=timezone)\n",
    "# Setting up altitude matrix\n",
    "min_height, max_height = 0, 1000 # Highest NRL MSISE density data is 1000 km\n",
    "d_height = 0.5 # Delta Z resolution desired (km)\n",
    "n_height = int(abs(max_height - min_height) / d_height) + 1 #Calculate number of height intervals\n",
    "alts = np.linspace(start=min_height,stop=max_height,num=n_height) \n",
    "\n",
    "# Radius of the Earth at launch site.\n",
    "# add in elevation function is height is relative instead of absolute.\n",
    "r_0 = get_earthradius(lat) # + elevation_function(lat,lon)\n",
    "\n",
    "# Importing Zenith Angle based on Location and DateTime.\n",
    "cosmu = np.cos(np.radians(90-zen_func(lat,lon,flight_datetime)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7c964-8fb3-4d98-aa44-6a15c4d5593a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2: Science Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918af96-ab58-4247-83d0-99fb66b503c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "minmax=[min_wave,max_wave]\n",
    "molmass =  [x/NA for x in molmass_g]\n",
    "n_mols = len(mols)\n",
    "\n",
    "# refractive index at ground, not accounted for, setting all to one (1).\n",
    "ri_0 = np.ones((n_mols))\n",
    "\n",
    "# Array for wavelengths\n",
    "n_waves = int((max_wave - min_wave) / d_wave + 1)\n",
    "waves = np.linspace(min_wave, max_wave, n_waves)\n",
    "\n",
    "absorp_data_all = getdata_xray(mols,minmax, molmass)\n",
    "# absorp_data_all, molmass = getdata_uvvis(mols,minmax, molmass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16017f6-73c2-42e8-84ac-2d2a87d2159c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3: Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf667dc-e5a5-4640-b4f9-dc3051aff28a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the atmospheric data at the specified lat, lon, time and altitudes\n",
    "density_data = msise_4d(flight_datetime,alts,lat,180-lon) #g/cm^3\n",
    "\n",
    "# Get the length of the data for the first molecule\n",
    "n_data = len(density_data[mols[0]].data.ravel())\n",
    "\n",
    "# Initialize a 2D array to store density for all molecules\n",
    "density_all = np.zeros((n_mols,n_data))\n",
    "# density_all_ratio = np.zeros((n_mols,n_data))\n",
    "\n",
    "# Loop over all molecules to store the data in the 2D array\n",
    "for i in range(n_mols):\n",
    "    density_all[i,:] = density_data[mols[i]].data.ravel()\n",
    "    # density_all_ratio[i,:] = density_all[i,:] / density_all[i,0]\n",
    "\n",
    "# Get the temperature and total density data from the atmospheric model and ravel it to a 1D array\n",
    "map_temp = density_data['Talt'].data.ravel()\n",
    "map_totalrho = density_data['rho'].data.ravel()\n",
    "\n",
    "# Formatting Absorption Cross-section into a 2D array based on altitude, temperature, and wavelength maps.\n",
    "absorp_spline = calc_absorp_spline(absorp_data_all,waves,map_temp, n_mols, n_waves, n_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0565f-3c99-4ace-af88-7be9b6bc66c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4: Calculating Absorption Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e4490-5b60-4878-93a8-6c9188e11299",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate the absorption by multiplying the spline and data of each molecule and height.\n",
    "absorp = np.zeros((n_mols, n_waves, n_height))\n",
    "for m in range(n_mols): # Loop over the number of molecules\n",
    "    for z in range(n_height): # Loop over the height\n",
    "        absorp[m, :, z] = absorp_spline[m, :, z] * density_all[m, z] # cm^-1\n",
    "\n",
    "# Calculate Differential Path Length\n",
    "DiffPathLength = calc_DiffPathLength(ri_0, r_0, d_height, cosmu, n_height)\n",
    "\n",
    "# Calculate Optical Depth\n",
    "OptDepth = calc_OptDepth(absorp, ri_0, density_all_ratio, d_height, cosmu)\n",
    "\n",
    "# Calculate Fraction of intensity (0 is fully absorbed, 1 is no absorption).\n",
    "Ifrac = np.exp(-OptDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90a881-1e85-4d7e-acd5-dfde0e6616e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the Transmission over time of the Flight Profile\n",
    "fp_num = len(fp_zall)\n",
    "Ifrac_time = np.zeros((n_waves, fp_num))\n",
    "for z in range(fp_num):\n",
    "    # Get the index of the height closest to the current altitude\n",
    "    z_i = np.max(np.where(alts <= fp_zall[z]))\n",
    "    # Assign the fraction of intensity at the current height\n",
    "    Ifrac_time[:, z] = Ifrac[:, z_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac5864-8061-4bca-978c-c3ff442ddf51",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pt = 32\n",
    "aspect = (16, 9)\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({'font.size': pt})\n",
    "fig, ax1 = plt.subplots(figsize=aspect)\n",
    "ax1.pcolormesh(fp_tall,waves,Ifrac_time,shading='auto')\n",
    "ax1.set_xlim(fp_tall[0],fp_tall[-1])\n",
    "ax1.set_ylim((waves[0],waves[-1]))\n",
    "ax1.set_ylabel('Wavelength (nm)')\n",
    "ax1.set_xlabel('T+ (s) after {} on {}'.format(flight_datetime.time(),flight_datetime.date()))\n",
    "plt.show(); plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514fda3-7652-48aa-a3cd-f02ae1684794",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot Density\n",
    "pt = 24\n",
    "aspect = (16, 9)\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=aspect)\n",
    "plt.rcParams.update({'font.size': pt})\n",
    "plt.ylabel('Altitude (km)')\n",
    "plt.xlabel('Density (kg/m$^3$)')\n",
    "\n",
    "Y = alts\n",
    "# ymin,ymax = 0, fp_zall.max()\n",
    "ymin,ymax = 100, 300\n",
    "plt.ylim((ymin, ymax))\n",
    "\n",
    "xmin,xmax = 1e-16, 1e-6\n",
    "plt.xlim((xmin, xmax))\n",
    "\n",
    "moleculeweight = np.array(molmass)/1000\n",
    "X = density_all * moleculeweight[:,None] * 100**3 # (molecules/cm^3)*(kg/molecule) * (100^3 cm^3/m^3) = kg/m^3\n",
    "X2 = np.sum(X,axis=0)\n",
    "X3 = map_totalrho* (100**3 / 1000) # Total density converted from g/cm^3 to kg/m^3\n",
    "percentlabel = np.sum(X[:,101:301],axis=1)/np.sum(X3[101:301])\n",
    "totallabel = np.sum(X[:,101:301])/np.sum(X3[101:301])\n",
    "\n",
    "for m in range(n_mols):\n",
    "    # percentlabel = np.mean(X[m,:]/X3)\n",
    "    label = mols[m]+' ({:.0%})'.format(percentlabel[m])\n",
    "    plt.semilogx(X[m,:], Y, '-', lw=2, label=label)\n",
    "\n",
    "plt.semilogx(X3, Y, '--k', lw=2, label='Total ({:.0%})'.format(totallabel))\n",
    "\n",
    "plt.title('Atmospheric Density (% Contribution between 100-300 km)')\n",
    "plt.legend(fontsize=pt,frameon=False)\n",
    "plt.show(); plt.close();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
